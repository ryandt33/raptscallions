# Fix Summary: Test Rejection Workflow

## Problem

Developer agent was implementing "testability hacks" to satisfy tests that used non-existent library APIs, instead of rejecting the tests back to the test writer.

**Example of the problem:**
```typescript
// Test (wrong API):
expect(users._).toBeDefined();  // Drizzle tables don't have `_` property

// Developer implemented hack:
export const users = pgTable('users', { ...columns });
Object.assign(users, { _: {} });  // ‚ùå Added fake property for tests!
```

## Root Cause

The developer agent **did** have rejection instructions, but they were:

1. **Buried mid-file** (line 61 of developer.md) - easy to miss
2. **Not front-and-center** - agent starts implementing before reading rejection rules
3. **Missing from command file** - `/implement` command didn't mention rejection
4. **Not emphasized enough** - competed with other "CRITICAL" sections

## What Was Fixed

### 1. Updated `/implement` Command (`.claude/commands/implement.md`)

**Added prominent CRITICAL section:**

```markdown
## CRITICAL: Test-API Mismatch Detection

‚ö†Ô∏è If you discover that tests were written with incorrect assumptions:

**DO NOT:**
- Hack the implementation to satisfy bad tests
- Add wrapper code just to make tests pass
- Use Object.assign tricks or test-only methods

**INSTEAD:**
1. Set workflow_state: TESTS_REVISION_NEEDED
2. Document the mismatch in History
3. Add detailed feedback in Reviews section
4. DO NOT implement anything
```

Now the command file itself warns about this at the top level.

### 2. Restructured Developer Agent Instructions (`.claude/agents/developer.md`)

**Moved rejection check to Phase 2 start:**

```markdown
### Phase 2: Implementation (TESTS_READY ‚Üí IMPLEMENTED)

## ‚ö†Ô∏è BEFORE YOU START IMPLEMENTING - READ THIS ‚ö†Ô∏è

**STEP 0: Validate Tests First**

Before writing ANY implementation code:
1. Read ALL test files completely
2. Verify tests use library APIs correctly
3. If ANY test uses non-existent API:
   - STOP IMMEDIATELY
   - DO NOT IMPLEMENT ANYTHING
   - See "üö´ Handling Test-API Mismatches" section
```

This forces validation BEFORE implementation starts.

**Created prominent rejection section:**

```markdown
## üö´ Handling Test-API Mismatches üö´

**CRITICAL REJECTION RULE:**

[Detailed rejection process with examples]
```

Now it's impossible to miss.

### 3. Enhanced `/write-tests` Command (`.claude/commands/write-tests.md`)

**Added revision process:**

```markdown
### Test Revision (TESTS_REVISION_NEEDED ‚Üí TESTS_READY)

If the task is in TESTS_REVISION_NEEDED state (developer rejected tests):
1. Read "Test Revision Required" feedback in Reviews section
2. Fix tests based on feedback
3. Use actual library APIs
```

**Added API validation section:**

```markdown
## CRITICAL: Use Real Library APIs

‚ö†Ô∏è Tests must use actual, documented library APIs - not imagined ones.

[Examples of good vs bad tests]
```

### 4. Created Comprehensive Documentation

**Created `TEST_REVISION_WORKFLOW.md`:**
- Complete workflow explanation
- Examples of valid/invalid rejections
- Benefits of the approach
- Monitoring and best practices

**Updated `WORKFLOW_COMPLETE.md`:**
- Added test rejection as part of Phase 6
- Documented the TESTS_REVISION_NEEDED flow
- Linked to detailed docs

## How It Works Now

### New Flow

```
1. Tests written (may have bad API assumptions)
   ‚Üì
2. Developer reads tests
   ‚Üì
3. ‚ö†Ô∏è VALIDATION CHECKPOINT ‚ö†Ô∏è
   Developer checks: Do tests use real library APIs?
   ‚Üì
   ‚îú‚îÄ YES ‚Üí Proceed with implementation
   ‚îÇ
   ‚îî‚îÄ NO ‚Üí REJECT
          ‚Üì
          Set TESTS_REVISION_NEEDED
          ‚Üì
          Document what's wrong
          ‚Üì
          DON'T IMPLEMENT ANYTHING
          ‚Üì
          Orchestrator calls /write-tests again
          ‚Üì
          Tests rewritten with correct APIs
          ‚Üì
          Back to step 2
```

### Key Changes

1. **Validation happens FIRST** - before any implementation code
2. **Rejection is prominent** - can't miss it in command or agent files
3. **Clear criteria** - examples of what qualifies as mismatch
4. **Documented process** - detailed instructions for rejection
5. **Loop handled** - orchestrator already has TESTS_REVISION_NEEDED state

## Example Scenario

### Before Fix

```
Agent: Reads tests that expect users._
Agent: Starts implementing
Agent: "Hmm, Drizzle doesn't have users._, but tests expect it"
Agent: "I'll add it with Object.assign to make tests pass"
Result: ‚ùå Implementation polluted with testability hack
```

### After Fix

```
Agent: BEFORE YOU START IMPLEMENTING - READ THIS ‚ö†Ô∏è
Agent: STEP 0: Validate Tests First
Agent: Reads tests that expect users._
Agent: Checks Drizzle docs/types
Agent: "users._ doesn't exist in Drizzle"
Agent: "This is a test-API mismatch"
Agent: Sets workflow_state: TESTS_REVISION_NEEDED
Agent: Documents: "Tests expect users._ but Drizzle tables don't have this"
Agent: Exits without implementing
Orchestrator: Sees TESTS_REVISION_NEEDED
Orchestrator: Calls /write-tests again
Agent: Reads rejection feedback
Agent: Rewrites tests to use users.id, users.email instead
Agent: Task back to TESTS_READY
Agent: Now implements cleanly without hacks
Result: ‚úÖ Clean implementation, correct tests
```

## Verification

To verify the fix works:

1. **Test with intentionally bad tests:**
   - Write tests expecting non-existent API
   - Run `/implement`
   - Verify agent rejects without implementing
   - Verify TESTS_REVISION_NEEDED state set
   - Verify feedback in Reviews section

2. **Test with corrected tests:**
   - Run `/write-tests` again
   - Verify tests rewritten
   - Verify task back to TESTS_READY
   - Run `/implement` again
   - Verify clean implementation

## Files Modified

1. `.claude/commands/implement.md` - Added CRITICAL rejection section
2. `.claude/commands/write-tests.md` - Added revision process and API validation
3. `.claude/agents/developer.md` - Restructured to validate tests BEFORE implementing
4. `docs/WORKFLOW_COMPLETE.md` - Documented test rejection path
5. `docs/TEST_REVISION_WORKFLOW.md` - Created comprehensive guide (new file)
6. `docs/FIX_SUMMARY.md` - This file (new)

## Orchestrator Already Had Support

The orchestrator already had the state machine set up correctly:

```typescript
const STATE_TRANSITIONS: Record<WorkflowState, WorkflowState | null> = {
  TESTS_READY: "IMPLEMENTING",
  TESTS_REVISION_NEEDED: "TESTS_READY",  // Already there!
  IMPLEMENTING: "IMPLEMENTED",
};

const STATE_COMMANDS: Record<WorkflowState, string | null> = {
  APPROVED: "write-tests",
  TESTS_READY: "implement",
  TESTS_REVISION_NEEDED: "write-tests",  // Already there!
};
```

The state was defined but agents weren't using it because instructions weren't clear enough.

## Benefits

### Code Quality
- No testability hacks in implementation
- No wrapper code added just for tests
- Clean, minimal implementations
- Codebase doesn't accumulate unnecessary complexity

### Test Quality
- Tests use actual library APIs
- Tests reflect reality, not wishes
- Better test coverage of real behavior
- Fewer false positives

### Developer Experience
- Clear rejection criteria
- Detailed feedback on what to fix
- Learning opportunity for test writers
- Faster implementation after tests corrected

### Workflow Integrity
- TDD principle maintained (tests guide implementation)
- Tests don't force bad architecture
- Quality gates work as intended
- Epic review won't find these issues (prevented upstream)

## Monitoring

Track effectiveness:

```bash
# Count test rejections
grep -r "TESTS_REVISION_NEEDED" backlog/tasks/

# View rejection reasons
grep -A 20 "Test Revision Required" backlog/tasks/*/*.md

# Check for testability hacks in code
grep -r "Object.assign.*_" packages/
```

High rejection rates indicate:
- Test writers need API training
- Specs should include API references
- Library docs might be unclear

Low hack rates (after fix) indicate:
- Rejection workflow is working
- Code quality maintained
- Tests improved

## Conclusion

The problem wasn't a missing feature - the `TESTS_REVISION_NEEDED` state and workflow loop already existed. The problem was that the rejection instructions weren't **prominent and early enough** in the agent's decision process.

**Key insight:** In TDD, the developer must validate tests BEFORE starting implementation. Making this a STEP 0 at the very beginning of Phase 2 ensures the agent can't start coding before checking tests.

By restructuring the instructions to put validation FIRST and making rejection rules PROMINENT, we ensure clean implementations without testability hacks.
