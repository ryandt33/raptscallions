---
id: "E05-T010"
title: "Integration tests and cleanup jobs"
status: "todo"
priority: "high"
labels: [backend, testing, workers]
assignee: ""
workflow_state: "DRAFT"
epic: "E05"
depends_on: ["E05-T006", "E05-T007"]
blocks: []
breakpoint: false
assigned_agent: ""
created_at: "2026-01-13T00:00:00Z"
updated_at: "2026-01-13T00:00:00Z"
spec_file: ""
test_files: []
code_files: []
pr_url: ""
pr_url: ""
---

# E05-T010: Integration tests and cleanup jobs

## Description

Implement comprehensive integration tests for file storage workflows (upload → download → delete) with multi-backend support verification. Create BullMQ background jobs for orphaned file cleanup (30+ days soft-deleted) and weekly quota reconciliation.

## Acceptance Criteria

### Integration Tests - Core Workflows
- [ ] AC1: Test complete upload → download → delete workflow
- [ ] AC2: Test upload with quota validation (success and quota exceeded)
- [ ] AC3: Test upload with file size validation (success and too large)
- [ ] AC4: Test signed URL generation and expiration
- [ ] AC5: Test file listing with pagination and filters
- [ ] AC6: Test soft delete updates quota correctly
- [ ] AC7: Test permission checks (owner, group admin, system admin)
- [ ] AC8: Test three-tier limit resolution (default → role → user override)

### Integration Tests - Multi-Backend
- [ ] AC9: Test S3/MinIO backend with real MinIO container
- [ ] AC10: Test local filesystem backend
- [ ] AC11: Test backend switching (upload to one, read from same)
- [ ] AC12: Verify storage key format consistency across backends
- [ ] AC13: Test error handling for backend failures
- [ ] AC14: Test signed URL validity for each backend

### Integration Tests - Avatars & Attachments
- [ ] AC15: Test setting user/group/class avatar
- [ ] AC16: Test removing avatar clears association but keeps file
- [ ] AC17: Test adding multiple attachments to assignment
- [ ] AC18: Test removing attachment clears association
- [ ] AC19: Test cascade behavior (delete entity → files remain)
- [ ] AC20: Test attachment limit enforcement (max 10 per assignment)

### Integration Tests - Admin Routes
- [ ] AC21: Test admin setting user limit override
- [ ] AC22: Test admin clearing user override
- [ ] AC23: Test admin setting group role limits
- [ ] AC24: Test non-admin forbidden from admin routes
- [ ] AC25: Test effective limits calculated correctly after override

### Cleanup Job - Orphaned Files
- [ ] AC26: OrphanedFileCleanupJob runs daily (cron: 0 2 * * *)
- [ ] AC27: Identifies files with status='soft_deleted' and deleted_at > 30 days ago
- [ ] AC28: Deletes file from storage backend
- [ ] AC29: Deletes file record from database
- [ ] AC30: Updates user quota (decrements used_bytes)
- [ ] AC31: Logs cleanup summary (files deleted, bytes freed)
- [ ] AC32: Handles storage backend errors gracefully (marks file for retry)

### Cleanup Job - Quota Reconciliation
- [ ] AC33: QuotaReconciliationJob runs weekly (cron: 0 3 * * 0)
- [ ] AC34: Recalculates used_bytes for all users from files table
- [ ] AC35: Compares with user_storage_limits.used_bytes
- [ ] AC36: Updates user_storage_limits if discrepancy found
- [ ] AC37: Logs discrepancies (user_id, expected, actual, diff)
- [ ] AC38: Sends alert if discrepancy > 100MB for any user
- [ ] AC39: Handles users with no storage limit record (creates default)

### Test Infrastructure
- [ ] AC40: Docker Compose test environment with MinIO
- [ ] AC41: Test fixtures for files, users, groups, classes
- [ ] AC42: Cleanup after tests (delete test files from storage)
- [ ] AC43: Integration tests can run against any backend
- [ ] AC44: Coverage report includes integration tests
- [ ] AC45: Tests run in CI/CD pipeline

### Monitoring & Observability
- [ ] AC46: OpenTelemetry traces for file operations
- [ ] AC47: Metrics: file_uploads_total, file_downloads_total, storage_bytes_used
- [ ] AC48: Metrics: quota_exceeded_total, cleanup_files_deleted_total
- [ ] AC49: Log structured events: file.uploaded, file.deleted, quota.exceeded
- [ ] AC50: Alert if cleanup job fails or finds >1000 orphaned files

## Technical Notes

### Integration Test - Complete Workflow

```typescript
// apps/api/src/__tests__/integration/files.workflow.test.ts

import { describe, it, expect, beforeAll, afterAll } from 'vitest';
import { createTestServer } from '../helpers/test-server.js';
import { createTestUser, loginAsUser } from '../helpers/test-users.js';
import FormData from 'form-data';
import fs from 'fs';

describe('File Storage Integration', () => {
  let app: any;
  let userToken: string;
  let userId: string;
  let fileId: string;

  beforeAll(async () => {
    app = await createTestServer();
    const { user, token } = await loginAsUser(app);
    userId = user.id;
    userToken = token;
  });

  afterAll(async () => {
    await app.close();
  });

  describe('Complete Upload → Download → Delete Workflow', () => {
    it('should upload, download, and delete file successfully', async () => {
      // 1. Upload file
      const form = new FormData();
      const testBuffer = Buffer.from('integration test content');
      form.append('file', testBuffer, 'test-integration.txt');
      form.append('entityType', 'avatar');

      const uploadResponse = await app.inject({
        method: 'POST',
        url: '/files',
        headers: {
          authorization: `Bearer ${userToken}`,
          ...form.getHeaders(),
        },
        payload: form,
      });

      expect(uploadResponse.statusCode).toBe(201);
      const uploadData = uploadResponse.json();
      fileId = uploadData.data.id;

      expect(uploadData.data).toMatchObject({
        name: 'test-integration.txt',
        mimeType: 'text/plain',
        sizeBytes: testBuffer.length,
      });

      // 2. Get file metadata with signed URL
      const getResponse = await app.inject({
        method: 'GET',
        url: `/files/${fileId}`,
        headers: { authorization: `Bearer ${userToken}` },
      });

      expect(getResponse.statusCode).toBe(200);
      const getData = getResponse.json();

      expect(getData.data).toHaveProperty('signedUrl');
      expect(getData.data.signedUrl).toMatch(/^https?:\/\//);

      // 3. Download file using signed URL
      const downloadResponse = await fetch(getData.data.signedUrl);
      const downloadedContent = await downloadResponse.text();

      expect(downloadedContent).toBe('integration test content');

      // 4. Verify quota increased
      const quotaResponse = await app.inject({
        method: 'GET',
        url: '/users/me/quota',
        headers: { authorization: `Bearer ${userToken}` },
      });

      expect(quotaResponse.statusCode).toBe(200);
      const quotaData = quotaResponse.json();
      expect(quotaData.data.usedBytes).toBeGreaterThanOrEqual(testBuffer.length);

      // 5. Soft delete file
      const deleteResponse = await app.inject({
        method: 'DELETE',
        url: `/files/${fileId}`,
        headers: { authorization: `Bearer ${userToken}` },
      });

      expect(deleteResponse.statusCode).toBe(204);

      // 6. Verify file is soft deleted (not in active list)
      const listResponse = await app.inject({
        method: 'GET',
        url: '/files',
        headers: { authorization: `Bearer ${userToken}` },
      });

      const listData = listResponse.json();
      const deletedFile = listData.data.find((f: any) => f.id === fileId);
      expect(deletedFile).toBeUndefined();

      // 7. Verify quota decreased after soft delete
      const quotaAfterResponse = await app.inject({
        method: 'GET',
        url: '/users/me/quota',
        headers: { authorization: `Bearer ${userToken}` },
      });

      const quotaAfterData = quotaAfterResponse.json();
      expect(quotaAfterData.data.usedBytes).toBe(quotaData.data.usedBytes - testBuffer.length);
    });

    it('should enforce quota limits', async () => {
      // Set user quota to 100 bytes
      await setUserQuota(userId, { limit: 100 });

      // Try to upload 200-byte file
      const form = new FormData();
      form.append('file', Buffer.alloc(200), 'large-file.bin');

      const response = await app.inject({
        method: 'POST',
        url: '/files',
        headers: {
          authorization: `Bearer ${userToken}`,
          ...form.getHeaders(),
        },
        payload: form,
      });

      expect(response.statusCode).toBe(422);
      expect(response.json()).toMatchObject({
        code: 'QUOTA_EXCEEDED',
        details: expect.objectContaining({
          usedBytes: expect.any(Number),
          quotaBytes: 100,
        }),
      });
    });

    it('should enforce file size limits', async () => {
      // Set user max file size to 50 bytes
      await setUserLimit(userId, { maxFileSizeBytes: 50 });

      // Try to upload 100-byte file
      const form = new FormData();
      form.append('file', Buffer.alloc(100), 'too-large.bin');

      const response = await app.inject({
        method: 'POST',
        url: '/files',
        headers: {
          authorization: `Bearer ${userToken}`,
          ...form.getHeaders(),
        },
        payload: form,
      });

      expect(response.statusCode).toBe(400);
      expect(response.json().error).toContain('exceeds limit');
    });
  });

  describe('Three-Tier Limit Resolution', () => {
    it('should use system defaults when no overrides', async () => {
      const limitsResponse = await app.inject({
        method: 'GET',
        url: '/users/me/limits',
        headers: { authorization: `Bearer ${userToken}` },
      });

      const limitsData = limitsResponse.json();
      expect(limitsData.data.source).toBe('system_default');
      expect(limitsData.data.maxFileSizeBytes).toBe(10485760); // 10MB default
    });

    it('should use role-based limits from group settings', async () => {
      // Set group role limits for teacher
      await setGroupRoleLimits(groupId, {
        teacher: {
          maxFileSizeBytes: 50 * 1024 * 1024, // 50MB
          storageQuotaBytes: 5 * 1024 * 1024 * 1024, // 5GB
        },
      });

      const limitsResponse = await app.inject({
        method: 'GET',
        url: '/users/me/limits',
        headers: { authorization: `Bearer ${teacherToken}` },
      });

      const limitsData = limitsResponse.json();
      expect(limitsData.data.source).toBe('role_based');
      expect(limitsData.data.maxFileSizeBytes).toBe(50 * 1024 * 1024);
    });

    it('should use user override when set by admin', async () => {
      // Admin sets user-specific override
      await app.inject({
        method: 'PATCH',
        url: `/admin/users/${userId}/limits`,
        headers: { authorization: `Bearer ${adminToken}` },
        payload: {
          maxFileSizeBytes: 100 * 1024 * 1024, // 100MB
          storageQuotaBytes: 10 * 1024 * 1024 * 1024, // 10GB
          reason: 'Graduate research project',
        },
      });

      const limitsResponse = await app.inject({
        method: 'GET',
        url: '/users/me/limits',
        headers: { authorization: `Bearer ${userToken}` },
      });

      const limitsData = limitsResponse.json();
      expect(limitsData.data.source).toBe('user_override');
      expect(limitsData.data.maxFileSizeBytes).toBe(100 * 1024 * 1024);
    });
  });

  describe('Multi-Backend Support', () => {
    it('should upload and download from MinIO backend', async () => {
      // Test already uses MinIO as default backend
      const form = new FormData();
      form.append('file', Buffer.from('test minio'), 'test-minio.txt');

      const uploadResponse = await app.inject({
        method: 'POST',
        url: '/files',
        headers: {
          authorization: `Bearer ${userToken}`,
          ...form.getHeaders(),
        },
        payload: form,
      });

      expect(uploadResponse.statusCode).toBe(201);

      const fileId = uploadResponse.json().data.id;

      // Download and verify
      const getResponse = await app.inject({
        method: 'GET',
        url: `/files/${fileId}`,
        headers: { authorization: `Bearer ${userToken}` },
      });

      expect(getResponse.statusCode).toBe(200);
      expect(getResponse.json().data.signedUrl).toContain('localhost:9000'); // MinIO endpoint
    });
  });
});
```

### Background Job - Orphaned File Cleanup

```typescript
// apps/worker/src/jobs/orphaned-file-cleanup.job.ts

import { Worker, Job } from 'bullmq';
import { db } from '@raptscallions/db';
import { files, userStorageLimits } from '@raptscallions/db/schema';
import { eq, and, lt, sql } from 'drizzle-orm';
import { StorageBackendFactory } from '@raptscallions/storage';
import { logger } from '@raptscallions/telemetry';

interface CleanupResult {
  filesDeleted: number;
  bytesFreed: number;
  errors: string[];
}

export async function runOrphanedFileCleanup(): Promise<CleanupResult> {
  const thirtyDaysAgo = new Date(Date.now() - 30 * 24 * 60 * 60 * 1000);

  logger.info({ thirtyDaysAgo }, 'Starting orphaned file cleanup');

  // Find soft-deleted files older than 30 days
  const orphanedFiles = await db.query.files.findMany({
    where: and(
      eq(files.status, 'soft_deleted'),
      lt(files.deletedAt, thirtyDaysAgo)
    ),
    limit: 1000, // Process in batches
  });

  logger.info({ count: orphanedFiles.length }, 'Found orphaned files');

  const backend = StorageBackendFactory.getBackend();
  const result: CleanupResult = {
    filesDeleted: 0,
    bytesFreed: 0,
    errors: [],
  };

  for (const file of orphanedFiles) {
    try {
      // Delete from storage backend
      await backend.delete(file.storageKey);

      // Delete from database
      await db.delete(files).where(eq(files.id, file.id));

      // Update user quota (decrement used_bytes)
      await db
        .update(userStorageLimits)
        .set({
          usedBytes: sql`${userStorageLimits.usedBytes} - ${file.sizeBytes}`,
        })
        .where(eq(userStorageLimits.userId, file.uploadedBy));

      result.filesDeleted++;
      result.bytesFreed += file.sizeBytes;

      logger.debug({ fileId: file.id, storageKey: file.storageKey }, 'Deleted orphaned file');
    } catch (error) {
      logger.error({ error, fileId: file.id }, 'Failed to delete orphaned file');
      result.errors.push(`${file.id}: ${(error as Error).message}`);
    }
  }

  logger.info(result, 'Orphaned file cleanup completed');

  return result;
}

// BullMQ Worker
export function createOrphanedFileCleanupWorker() {
  return new Worker(
    'orphaned-file-cleanup',
    async (job: Job) => {
      const result = await runOrphanedFileCleanup();

      if (result.errors.length > 0) {
        throw new Error(`Cleanup completed with ${result.errors.length} errors`);
      }

      return result;
    },
    {
      connection: {
        host: process.env.REDIS_HOST || 'localhost',
        port: parseInt(process.env.REDIS_PORT || '6379'),
      },
    }
  );
}
```

### Background Job - Quota Reconciliation

```typescript
// apps/worker/src/jobs/quota-reconciliation.job.ts

import { Worker, Job } from 'bullmq';
import { db } from '@raptscallions/db';
import { files, users, userStorageLimits } from '@raptscallions/db/schema';
import { eq, sum, and } from 'drizzle-orm';
import { logger } from '@raptscallions/telemetry';

interface ReconciliationResult {
  usersChecked: number;
  discrepancies: Array<{
    userId: string;
    expected: number;
    actual: number;
    diff: number;
  }>;
  usersUpdated: number;
}

const ALERT_THRESHOLD_BYTES = 100 * 1024 * 1024; // 100MB

export async function runQuotaReconciliation(): Promise<ReconciliationResult> {
  logger.info('Starting quota reconciliation');

  const result: ReconciliationResult = {
    usersChecked: 0,
    discrepancies: [],
    usersUpdated: 0,
  };

  // Get all users
  const allUsers = await db.query.users.findMany();

  for (const user of allUsers) {
    result.usersChecked++;

    // Calculate actual usage from files table
    const actualUsage = await db
      .select({ total: sum(files.sizeBytes) })
      .from(files)
      .where(
        and(
          eq(files.uploadedBy, user.id),
          eq(files.status, 'active')
        )
      );

    const actualBytes = Number(actualUsage[0]?.total || 0);

    // Get recorded usage from user_storage_limits
    const limitRecord = await db.query.userStorageLimits.findFirst({
      where: eq(userStorageLimits.userId, user.id),
    });

    const recordedBytes = limitRecord?.usedBytes || 0;

    // Check for discrepancy
    if (actualBytes !== recordedBytes) {
      const diff = Math.abs(actualBytes - recordedBytes);

      result.discrepancies.push({
        userId: user.id,
        expected: actualBytes,
        actual: recordedBytes,
        diff,
      });

      // Update user_storage_limits
      if (limitRecord) {
        await db
          .update(userStorageLimits)
          .set({ usedBytes: actualBytes })
          .where(eq(userStorageLimits.userId, user.id));
      } else {
        // Create default record
        await db.insert(userStorageLimits).values({
          userId: user.id,
          usedBytes: actualBytes,
        });
      }

      result.usersUpdated++;

      logger.warn(
        { userId: user.id, expected: actualBytes, actual: recordedBytes, diff },
        'Quota discrepancy found and corrected'
      );

      // Alert if large discrepancy
      if (diff > ALERT_THRESHOLD_BYTES) {
        logger.error(
          { userId: user.id, diff },
          'Large quota discrepancy detected (>100MB)'
        );
        // TODO: Send alert to monitoring system
      }
    }
  }

  logger.info(result, 'Quota reconciliation completed');

  return result;
}

// BullMQ Worker
export function createQuotaReconciliationWorker() {
  return new Worker(
    'quota-reconciliation',
    async (job: Job) => {
      return await runQuotaReconciliation();
    },
    {
      connection: {
        host: process.env.REDIS_HOST || 'localhost',
        port: parseInt(process.env.REDIS_PORT || '6379'),
      },
    }
  );
}
```

### Job Scheduling

```typescript
// apps/worker/src/scheduler.ts

import { Queue } from 'bullmq';
import { logger } from '@raptscallions/telemetry';

export async function scheduleRecurringJobs() {
  const connection = {
    host: process.env.REDIS_HOST || 'localhost',
    port: parseInt(process.env.REDIS_PORT || '6379'),
  };

  // Orphaned File Cleanup - Daily at 2 AM
  const cleanupQueue = new Queue('orphaned-file-cleanup', { connection });
  await cleanupQueue.add(
    'cleanup',
    {},
    {
      repeat: {
        pattern: '0 2 * * *', // Daily at 2 AM
      },
    }
  );

  logger.info('Scheduled orphaned file cleanup job (daily 2 AM)');

  // Quota Reconciliation - Weekly on Sunday at 3 AM
  const reconciliationQueue = new Queue('quota-reconciliation', { connection });
  await reconciliationQueue.add(
    'reconciliation',
    {},
    {
      repeat: {
        pattern: '0 3 * * 0', // Weekly on Sunday at 3 AM
      },
    }
  );

  logger.info('Scheduled quota reconciliation job (weekly Sunday 3 AM)');
}
```

### Docker Compose Test Environment

```yaml
# docker-compose.test.yml

version: '3.8'

services:
  postgres:
    image: postgres:16
    environment:
      POSTGRES_DB: raptscallions_test
      POSTGRES_USER: test
      POSTGRES_PASSWORD: test
    ports:
      - "5433:5432"

  redis:
    image: redis:7
    ports:
      - "6380:6379"

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9001:9000"  # API (use different port to avoid conflict)
      - "9002:9001"  # Console
    volumes:
      - test_minio_data:/data

volumes:
  test_minio_data:
```

### Test Helper - Cleanup

```typescript
// apps/api/src/__tests__/helpers/storage-cleanup.ts

import { StorageBackendFactory } from '@raptscallions/storage';
import { db } from '@raptscallions/db';
import { files } from '@raptscallions/db/schema';
import { like } from 'drizzle-orm';

/**
 * Clean up test files from storage and database
 */
export async function cleanupTestFiles(testPrefix: string = 'test-') {
  const backend = StorageBackendFactory.getBackend();

  // Find test files in database
  const testFiles = await db.query.files.findMany({
    where: like(files.name, `${testPrefix}%`),
  });

  // Delete from storage backend
  for (const file of testFiles) {
    try {
      await backend.delete(file.storageKey);
    } catch (error) {
      // Ignore errors (file may not exist in storage)
    }
  }

  // Delete from database
  await db.delete(files).where(like(files.name, `${testPrefix}%`));
}
```

## Out of Scope

- File integrity verification (checksums)
- Storage cost analytics
- File deduplication
- Automated backups
- Disaster recovery procedures
- Performance benchmarking suite
- Load testing for concurrent uploads

## History

| Date | State | Agent | Notes |
| ---- | ----- | ----- | ----- |
| 2026-01-13 | DRAFT | pm | Task created for Epic E05 |
